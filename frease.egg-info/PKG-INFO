Metadata-Version: 2.2
Name: frease
Version: 0.1.0
Summary: A frenchy package, that starts with a shallow PyTorch model and progressively adds layers 
Author-email: Clustery <bigarnaque@gmail.com>
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: datasets>=3.2.0
Requires-Dist: matplotlib>=3.10.0
Requires-Dist: rich>=13.9.4
Requires-Dist: setuptools>=75.8.0
Requires-Dist: torch>=2.6.0

IceCubes

How to use it:

training_recipe = ProgressiveRecipes(model) # this split your model main module list into ice cubes
training_recipe.progressive_simple(
    epochs, lr, group_size, global_trainning, scaling_factor
) # this gives the recipe to follow during training (typically adding layers progressively and freezing the ones that have already been trained)
trainer = ProgressiveTrainer(training_recipe) # this creates a trainer aware of the recipe (it will maybe be called 'cooker' in the future)
trainer.train(data_loader, optimizer, criterion) # this trains the model using (you need to have a data_loader giving pairs of X and y values)

You can also have the following recipe which starts with all layers and unfreeze only one layer at a time:
training_recipe.iterative_freeze_defreeze(
   epochs, lr, group_size, global_trainning, scaling_factor
)

I'm starting to love this package. It's the ultimate way to make large nn converge super fast. Probably making it reach local minima, but hell it is quick.


TODO:
Recipe: Faire en sorte d'ajuster le recipe si on a des layers sans poids -> dans ce cas il faut les ajouter avec le layer précédent car il est inutile d'apprendre

WARNING: Attention le loading d'un modèle ne fonctionne probablement pas (il n'a pas été testé en condition réelle)
